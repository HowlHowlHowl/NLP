{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HowlHowlHowl/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmOoKjBTlY4i"
      },
      "outputs": [],
      "source": [
        "# GITHUB REPO: https://github.com/iiscleap/Coswara-Data\n",
        "# PAPER: https://arxiv.org/pdf/2005.10548.pdf\n",
        "!git clone https://github.com/iiscleap/Coswara-Data.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEzXUfz7uG6w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "coswara_data_dir = os.path.abspath('.')+ '/Coswara-Data'  # Percorso locale della directory dei dati di Coswara\n",
        "extracted_data_dir = os.path.abspath('.')+ '/Extracted-Data'\n",
        "drive_data_dir = os.path.abspath('.')+ '/drive/MyDrive/Extracted-Data'\n",
        "\n",
        "print(\"Extracted data dir\", extracted_data_dir)\n",
        "print(\"Coswara data dir\", coswara_data_dir)\n",
        "\n",
        "if not os.path.exists(coswara_data_dir):\n",
        "    raise FileNotFoundError(\"Check the Coswara dataset directory!\")\n",
        "\n",
        "if not os.path.exists(extracted_data_dir):\n",
        "    os.makedirs(extracted_data_dir)\n",
        "\n",
        "dirs_extracted = set(map(os.path.basename, glob.glob('{}/202*'.format(extracted_data_dir))))\n",
        "dirs_all = set(map(os.path.basename, glob.glob('{}/202*'.format(coswara_data_dir))))\n",
        "\n",
        "dirs_to_extract = list(set(dirs_all) - dirs_extracted)\n",
        "\n",
        "for d in dirs_to_extract:\n",
        "    tar_cmd = 'cat {}/{}/*.tar.gz.* | tar -xvz -C {}'.format(coswara_data_dir, d, extracted_data_dir)\n",
        "    print(\"Executing command:\", tar_cmd)\n",
        "    p = subprocess.Popen(tar_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = p.communicate()\n",
        "    print(\"Standard output:\", stdout.decode())\n",
        "    print(\"Standard error:\", stderr.decode())\n",
        "\n",
        "    test_file_path = os.path.join(extracted_data_dir, 'test.txt')\n",
        "\n",
        "print(\"Extraction process complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjXxMVx48jeH"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/Extracted-Data /content/drive/MyDrive/Extracted-Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrl7VSTlPQXm"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# Delete the repository after data extraction\n",
        "# shutil.rmtree(coswara_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLqZSjSRAUsk"
      },
      "source": [
        "Una volta eseguiti i passaggi precedenti NON devono essere compilati mai più"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkE2vg2gBRfU"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "def statistic_obj(array):\n",
        "    q3, q1 = np.percentile(array, [75, 25])\n",
        "    return dict({\n",
        "        'mean': float(np.mean(array)),\n",
        "        'std_dev': float(np.std(array)),\n",
        "        'min': float(np.min(array)),\n",
        "        'max': float(np.max(array)),\n",
        "        'median': float(np.median(array)),\n",
        "        'skew': float(skew(array, axis=None)),\n",
        "        'kurt': float(kurtosis(array, axis=None)),\n",
        "        'perc25': float(q1),\n",
        "        'perc75': float(q3),\n",
        "        'root_mean_sqr': float(np.sqrt(np.mean(array ** 2))),\n",
        "        'iqr': float(q3 - q1)\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pba3sFfFBN3k"
      },
      "outputs": [],
      "source": [
        "def extract_statistical_features(vec, name='None'):\n",
        "    # prendiamo i primi 13 array e per ogni variante di mfcc\n",
        "    if name == 'mfcc' or name == 'mfcc_d' or name == 'mfcc_d2':\n",
        "        vec_features = []\n",
        "        # ne calcoliamo gli indicatori statistici\n",
        "        for i, array in enumerate(vec[0:13]):\n",
        "            # scarto interquartile\n",
        "            vec_features.append(statistic_obj(array))\n",
        "        return vec_features\n",
        "    else:\n",
        "        return statistic_obj(vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XMkvOoaBG0-"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def extract_features(audio, sr, category_name):\n",
        "    onset_env = librosa.onset.onset_strength(y=audio, sr=sr)\n",
        "\n",
        "    trim, index = librosa.effects.trim(y=audio)\n",
        "    duration = librosa.get_duration(y=trim)\n",
        "    # print('duration', duration)\n",
        "\n",
        "    # times = librosa.times_like(onset_env, sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
        "\n",
        "    # print('onset_frames:', onset_frames)\n",
        "\n",
        "    # Scommentare 1 comentare 2, run di quello grosso e di nuovo scommentare 2 e commentre 1\n",
        "    #tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
        "    tempo = librosa.feature.rhythm.tempo(onset_envelope=onset_env, sr=sr)\n",
        "    # print('tempo:', tempo[0])\n",
        "\n",
        "    period = np.argmax(librosa.stft(audio))\n",
        "    # print('period:', period)\n",
        "\n",
        "    rmse = librosa.feature.rms(y=audio)[0]\n",
        "    rmse_stat = extract_statistical_features(rmse, 'rmse')\n",
        "    # print('rmse:', np.shape(rmse))\n",
        "\n",
        "    sc = librosa.feature.spectral_centroid(y=audio)\n",
        "    sc_stat = extract_statistical_features(sc, 'sc')\n",
        "    # print('spectral centroid:' + str(np.shape(sc[0])) + ' ' + str(np.shape(sc)))\n",
        "\n",
        "    roll_off = librosa.feature.spectral_rolloff(y=audio, sr=sr, roll_percent=0.85)\n",
        "    roll_off_stat = extract_statistical_features(roll_off, 'roll_off')\n",
        "    # print('roll_off:' + str(np.shape(roll_off[0])) + ' ' + str(np.shape(roll_off)))\n",
        "\n",
        "    zc = librosa.feature.zero_crossing_rate(y=audio)\n",
        "    zc_stat = extract_statistical_features(zc, 'zc')\n",
        "    # print('zero crossing:' + str(np.shape(zc[0])) + ' ' + str(np.shape(zc)))\n",
        "\n",
        "    # la funzione mfcc restituisce matrici che stenderemo e ne calcoleremo gli indicatori statistici\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "    mfcc_feat = extract_statistical_features(mfcc, 'mfcc')\n",
        "\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_feat_d = extract_statistical_features(mfcc_delta, 'mfcc_d')\n",
        "\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    mfcc_feat_d2 = extract_statistical_features(mfcc_delta2, 'mfcc_d2')\n",
        "\n",
        "    features = {\n",
        "        category_name + 'duration': float(duration),\n",
        "        category_name + 'tempo': float(tempo[0]),\n",
        "        category_name + 'period': float(period),\n",
        "        category_name + 'onset': len(onset_frames),\n",
        "        category_name + 'rmse': rmse_stat,\n",
        "        category_name + 'sc': sc_stat,\n",
        "        category_name + 'roll_off': roll_off_stat,\n",
        "        category_name + 'zc': zc_stat,\n",
        "        category_name + 'mfcc': mfcc_feat,\n",
        "        category_name + 'mfcc_d': mfcc_feat_d,\n",
        "        category_name + 'mfcc_d2': mfcc_feat_d2,\n",
        "    }\n",
        "\n",
        "    # This piece of code expand these arrays from\n",
        "    # 'name': { 'stat_1': stat_1, ..., 'stat_n': stat_n }\n",
        "    # to\n",
        "    # 'name_stat_1': stat_1, ...,'name_stat_n': stat_n\n",
        "    for name in [category_name + 'rmse', category_name + 'sc', category_name + 'roll_off', category_name + 'zc']:\n",
        "        for key, value in features[name].items():\n",
        "            # Split the original key name into separate parts\n",
        "            parts = re.findall(r'[a-zA-Z]+|\\d+', key)\n",
        "            # Build the new key name by joining the parts with underscores\n",
        "            new_key = \"_\".join([name] + parts)\n",
        "            # Add the new key and value to the output dictionary\n",
        "            features[new_key] = value\n",
        "        del features[name]\n",
        "\n",
        "    # This code perform the flattening of the mfcc-like matrices\n",
        "    for name in [category_name + 'mfcc', category_name + 'mfcc_d', category_name + 'mfcc_d2']:\n",
        "        for i, element in enumerate(features[name]):\n",
        "            for key in element:\n",
        "                new_key = name+'_'+str(i)+'_'+key\n",
        "                features[new_key] = element[key]\n",
        "        del features[name]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSP84GCckZWf"
      },
      "outputs": [],
      "source": [
        "def extraction(prefix, file_path):\n",
        "    audio, sr = librosa.core.load(file_path)\n",
        "    audio, _ = librosa.effects.trim(audio)\n",
        "    return extract_features(audio, sr, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlABQySezlFS",
        "outputId": "77c7bde8-f806-42d1-a53b-3e60be6c65bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy-kur-UvgNh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Percorso completo del file di output nel tuo Google Drive\n",
        "output_file = os.path.join(os.path.abspath('.') + '/drive/MyDrive/Extracted-Data/', 'dataset_nuovo.json')\n",
        "# Percorso completo del file di errori nel tuo Google Drive\n",
        "errors_file = os.path.join(os.path.abspath('.') + '/drive/MyDrive/Extracted-Data/', 'errors.txt')\n",
        "\n",
        "def save_dataset(dataset):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(dataset, f, indent=4)\n",
        "\n",
        "def save_errors(errors):\n",
        "    with open(errors_file, 'w') as f:\n",
        "        json.dump(dataset, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VMdpVS-8Sr7"
      },
      "outputs": [],
      "source": [
        "# Percorso della directory principale\n",
        "main_dir = os.path.abspath('.') + '/drive/MyDrive/Extracted-Data/Extracted-Data'\n",
        "\n",
        "# Conteggio del numero totale di sottodirectory 202___\n",
        "total_subdirs_202 = sum(1 for _ in os.scandir(main_dir) if _.is_dir() and _.name.startswith(\"202\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwRtcb5epHCq"
      },
      "source": [
        "\n",
        "# NON ESEGUIRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMpqBAeKySvT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Metadata information: age, gender, location (country, state/province),\n",
        "# current health status (healthy / exposed / cured / infected) and the presence\n",
        "# of co-morbidity (pre-existing medical conditions).\n",
        "\n",
        "dataset = []\n",
        "errors = []\n",
        "\n",
        "# Variabili contatore per il progresso\n",
        "processed_subdirs_202 = 0\n",
        "processed_samples = 0\n",
        "\n",
        "# Iterazione su tutte le sottodirectory 202___\n",
        "for root, dirs, files in os.walk(main_dir):\n",
        "    for subdir in dirs:\n",
        "        if subdir.startswith(\"202\"):  # Solo se il nome della sottodirectory inizia con \"202\"\n",
        "            processed_subdirs_202 += 1\n",
        "            print(\"> Parsing subdir\", subdir, f\"({processed_subdirs_202}/{total_subdirs_202})\")\n",
        "            subdir_path = os.path.join(root, subdir)\n",
        "\n",
        "            # Conteggio del numero totale di sottocartelle all'interno di questa sottodirectory 202___\n",
        "            total_sample_dirs = sum(1 for _ in os.scandir(subdir_path) if _.is_dir())\n",
        "            processed_sample_dirs = 0\n",
        "            # Iterazione su tutte le sottocartelle\n",
        "            for subroot, subdirs, subfiles in os.walk(subdir_path):\n",
        "                for subsubdir in subdirs:\n",
        "                    processed_sample_dirs += 1\n",
        "                    sample_dir_path = os.path.join(subroot, subsubdir)\n",
        "\n",
        "                    # Inizializzazione delle variabili\n",
        "                    covid = None\n",
        "                    audio_features = {}\n",
        "\n",
        "                    print(f\"> > Parsing {os.path.basename(sample_dir_path)} - sample: {processed_sample_dirs}/{total_sample_dirs}\")\n",
        "\n",
        "                    # Estrazione dei dati da ogni file nella sottocartella Sample_id\n",
        "                    for file_name in os.listdir(sample_dir_path):\n",
        "                        file_path = os.path.join(sample_dir_path, file_name)\n",
        "\n",
        "                        if file_name.endswith('.json'):\n",
        "                          try:\n",
        "                            with open(file_path, 'r') as f:\n",
        "                                data = json.load(f)\n",
        "                                covid = data['covid_status']\n",
        "                                audio_features.update({\n",
        "                                    'age': data['a'],\n",
        "                                    'gender': data['g']\n",
        "                                })\n",
        "\n",
        "                          except Exception as e:\n",
        "                                err = (f\"< < Error processing metadata of {os.path.basename(sample_dir_path)} -> { file_path.split('/')[7]}:\\n\\t{str(e)}\" )\n",
        "                                print(err)\n",
        "                                errors.append(err)\n",
        "                        else:\n",
        "                            # print(\"calling extraction on\", file_name)\n",
        "                            # Esegui l'estrazione delle features per ogni file\n",
        "                            try:\n",
        "                                # Esegui l'estrazione delle features per ogni file\n",
        "                                feature_name = os.path.splitext(file_name)[0]\n",
        "                                audio_features.update(extraction(feature_name+'_', file_path))\n",
        "                            except Exception as e:\n",
        "                                err = (f\"< < Error processing sample {subdir} - { file_path.split('/')[7]}: file {file_path.split('/')[8]}\\n\\t{str(e)}\" )\n",
        "                                print(err)\n",
        "                                errors.append(err)\n",
        "                    processed_samples += 1\n",
        "                    dataset.append({\n",
        "                        'sample': os.path.basename(sample_dir_path),\n",
        "                        'covid': covid,\n",
        "                        'features': audio_features\n",
        "                    })\n",
        "\n",
        "            # Print per avvisare che hai finito con la subdir\n",
        "            print(f\"{processed_samples} processed samples\")\n",
        "            print(f\"> Finished parsing subdir: {subdir}... Saving dataset and errors.\")\n",
        "\n",
        "            save_dataset(dataset)\n",
        "            save_errors(errors)\n",
        "\n",
        "# Stampa il numero totale di subdir 202___\n",
        "print(\"Total 202___ subdirectories:\", total_subdirs_202)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/dataset.json', 'r') as file:\n",
        "    raw_data = json.load(file)"
      ],
      "metadata": {
        "id": "rgqwdHAxW3Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxEFaUipNMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4accea6f-2eb4-4524-e87a-cde13b130352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "data_file = os.path.join(os.path.abspath('.') + '/drive/MyDrive/Extracted-Data/', 'dataset.json')\n",
        "\n",
        "with open(data_file, 'r') as file:\n",
        "    raw_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJm3UJbez7nP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c94941b-fbcb-44f0-b0aa-afc225835934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conteggi di 'covid': {\n",
            "    \"healthy\": 1481,\n",
            "    \"positive_mild\": 426,\n",
            "    \"positive_moderate\": 165,\n",
            "    \"no_resp_illness_exposed\": 282,\n",
            "    \"positive_asymp\": 90,\n",
            "    \"recovered_full\": 146,\n",
            "    \"resp_illness_not_identified\": 159\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Inizializza un dizionario per contare le occorrenze di ciascun valore di 'covid'\n",
        "conteggio_covid = {}\n",
        "\n",
        "for elemento in raw_data:\n",
        "    valore_covid = elemento['covid']\n",
        "    if valore_covid in conteggio_covid:\n",
        "        conteggio_covid[valore_covid] += 1\n",
        "    else:\n",
        "        conteggio_covid[valore_covid] = 1\n",
        "\n",
        "# Stampa i conteggi di ciascun valore di 'covid'\n",
        "print(\"Conteggi di 'covid':\", json.dumps(conteggio_covid, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs3YKkCrYgoj"
      },
      "source": [
        "Qui riduciamo da 7 classi a 2 (perdendone 1 ovvero resp_illness_not_identified = #159) ovvero positivi e negativi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9kJhPvYhukb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38224b6-0ff3-44f4-c4d8-3bb922fde70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positives = 681\n",
            "Negatives = 1909\n",
            "Original data was  2749\n",
            "New data is 2590\n"
          ]
        }
      ],
      "source": [
        "positives = []\n",
        "negatives = []\n",
        "\n",
        "for el in raw_data:\n",
        "    if isinstance(el, dict):  # Verifica se l'elemento è un dizionario\n",
        "        if \"covid\" in el and isinstance(el[\"covid\"], str):  # Verifica se la chiave \"covid\" è presente e se il suo valore è una stringa\n",
        "            if el[\"covid\"] in ['healthy', 'recovered_full', 'no_resp_illness_exposed']:\n",
        "                el[\"covid\"] = \"negatives\"\n",
        "                negatives.append(el)\n",
        "            elif el[\"covid\"] != 'resp_illness_not_identified':\n",
        "                el[\"covid\"] = \"positives\"\n",
        "                positives.append(el)\n",
        "        else:\n",
        "            print(\"Warning: Element does not contain 'covid' key or 'covid' value is not a string:\", el)\n",
        "    else:\n",
        "        print(\"Warning: Element is not a dictionary:\", el)\n",
        "\n",
        "print(f\"Positives = {len(positives)}\")\n",
        "print(f\"Negatives = {len(negatives)}\")\n",
        "positives.extend(negatives)\n",
        "data = positives\n",
        "print('Original data was ',len(raw_data))\n",
        "print('New data is' , len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IeJvoQ6YXgy"
      },
      "source": [
        "Qui eliminamo i sample che contengono almeno un NaN fra le features (tutti i dati devono essere uguali in formato)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CT3Gq4H00Zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3dff4f-0542-4496-9c58-a1df37cadf04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New data without NaN 2492\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Estrai le features e il target da ogni elemento del dataset\n",
        "features_list = []\n",
        "target_list = []\n",
        "\n",
        "# Filtra i sample che non contengono valori NaN nelle featuresimport json\n",
        "filtered_data = []\n",
        "for entry in data:\n",
        "    has_missing_values = any(pd.isna(value) for key, value in entry['features'].items() if key != 'gender' and isinstance(value, (int, float)))\n",
        "    if not has_missing_values:\n",
        "        filtered_data.append(entry)\n",
        "print(\"New data without NaN\", len(filtered_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL351cdZYLlt"
      },
      "source": [
        "Qui eliminamo i dati che non hanno lo stesso numero di feature degli altri (file audio corrotti/ impossibile elaborarli, ad esdempio alcuni audio hanno lunghezza 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jZXY-TGihzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea91fe83-d36c-471d-dd65-7331c757ea05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sample list length: 2492\n",
            "Original target list length: 2492\n",
            "New sample list length: 2432\n",
            "New target list length: 2432\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Estrai le features e il target\n",
        "features_list = []\n",
        "target_list = []\n",
        "\n",
        "for entry in filtered_data:\n",
        "  # Converti il valore del campo 'gender' da stringa a 0 per 'female' e 1 per 'male'\n",
        "    gender_value = 1 if entry['features']['gender'] == 'male' else 0\n",
        "\n",
        "    # Aggiorna il campo 'gender' nel dizionario delle features\n",
        "    entry['features']['gender'] = gender_value\n",
        "\n",
        "    features_list.append(entry['features'])\n",
        "    target_list.append(entry['covid'])\n",
        "\n",
        "# Lista degli indici da rimuovere\n",
        "to_remove = []\n",
        "for i, c in enumerate(features_list):\n",
        "    if len(c) != 4295:\n",
        "        to_remove.append(i)\n",
        "\n",
        "print(\"Original sample list length:\", len(features_list))\n",
        "print(\"Original target list length:\", len(target_list))\n",
        "\n",
        "# Rimuovi gli elementi dalla lista in ordine inverso\n",
        "for idx in sorted(to_remove, reverse=True):\n",
        "    del features_list[idx]\n",
        "    del target_list[idx]\n",
        "\n",
        "print(\"New sample list length:\", len(features_list))\n",
        "print(\"New target list length:\", len(target_list))\n",
        "\n",
        "\n",
        "features = pd.DataFrame(features_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSu8QUndK3xY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Codifica le etichette del target\n",
        "label_encoder = LabelEncoder()\n",
        "target_encoded = label_encoder.fit_transform(target_list)\n",
        "# Ottieni i nomi delle classi originali\n",
        "classes_names = label_encoder.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYyJ_8rBJb95"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Calcola la matrice di correlazione tra le features\n",
        "correlation_matrix = features.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7qnSkjNZTBB"
      },
      "source": [
        "Raggruppiamo in insiemi le features che sono correlate fra loro almeno al 90% in quanto ridontanti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRYpp7J2PsK6"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Identifica i set di features strettamente correlate\n",
        "corr_threshold = 0.90\n",
        "corr_sets = defaultdict(set)  # Dizionario per memorizzare i set di features correlate\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > corr_threshold:\n",
        "            corr_sets[i].add(correlation_matrix.columns[i])\n",
        "            corr_sets[i].add(correlation_matrix.columns[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8crvUxkZiLk"
      },
      "outputs": [],
      "source": [
        "n_features_to_keep = 1000  # Specifica il numero di feature da mantenere\n",
        "n_top_to_keep = 500 # numero di feature con cui terminare\n",
        "proba_threshold = 0.2\n",
        "xgboost_threshold = 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUqVxK3NZaB3"
      },
      "source": [
        "Eliminamo suddette features lasciandone 1 a set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1hpYgVhWmXI"
      },
      "outputs": [],
      "source": [
        "# Crea un insieme per tenere traccia delle feature selezionate\n",
        "selected_features = set()\n",
        "\n",
        "# Ciclo su ogni insieme di features correlate\n",
        "for corr_set in corr_sets.values():\n",
        "    # Calcola la correlazione di ogni feature nel set con il target\n",
        "    corr_with_target = {feature: abs(features[feature].corr(pd.Series(target_encoded))) for feature in corr_set}\n",
        "    # Ordina le feature in base alla correlazione con il target\n",
        "    sorted_features = sorted(corr_with_target.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Seleziona solo le prime n_features_to_keep\n",
        "    top_features = [feature for feature, _ in sorted_features[:n_features_to_keep]]\n",
        "    # Aggiungi le top feature selezionate all'insieme selected_features\n",
        "    selected_features.update(top_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5hUXIuhRz0J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calcola la correlazione delle feature selezionate con il target\n",
        "correlation_with_target = features[selected_features].apply(lambda x: abs(x.corr(pd.Series(target_encoded))))\n",
        "# Calcola la correlazione delle feature selezionate con il target\n",
        "correlation_with_target = correlation_with_target.nlargest(n_features_to_keep)\n",
        "\n",
        "# Ottieni il nome delle feature selezionate\n",
        "selected_features = correlation_with_target.index\n",
        "\n",
        "# Aggiorna il DataFrame features_selected per includere solo le feature selezionate\n",
        "features_selected = features[selected_features]\n",
        "\n",
        "# Stampa la correlazione con il target per le feature selezionate\n",
        "print(correlation_with_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx0_VzGhU-ho"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "# Dividi il dataset in set di training, validation e test\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(features_selected, target_encoded, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI6yr-SMgFYY"
      },
      "source": [
        "SVM SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zx0aeVFU9qu"
      },
      "outputs": [],
      "source": [
        "# Addestramento con validazione incrociata per trovare i migliori iperparametri\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 1, 10]}\n",
        "svm = SVC(probability=True)\n",
        "clf = GridSearchCV(svm, parameters, cv=5)\n",
        "clf.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Valutazione sul set di validazione\n",
        "best_model = clf.best_estimator_\n",
        "y_pred_val_prob = best_model.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_val = (y_pred_val_prob >= proba_threshold).astype(int)  # Applicazione della soglia\n",
        "val_report = classification_report(y_val, y_pred_val, target_names=classes_names)\n",
        "print(\"Classification Report on validation set:\")\n",
        "print(val_report)\n",
        "\n",
        "# Addestramento sul set di addestramento e di validazione combinato\n",
        "best_model.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Valutazione finale sul set di test\n",
        "y_pred_test_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_test = (y_pred_test_prob >= proba_threshold).astype(int)  # Applicazione della soglia\n",
        "test_report = classification_report(y_test, y_pred_test, target_names=classes_names)\n",
        "print(\"Classification Report on test set:\")\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXEP87N9gHqi"
      },
      "source": [
        "LR SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmX3xbaFVWVl"
      },
      "outputs": [],
      "source": [
        "# Addestramento con validazione incrociata per trovare i migliori iperparametri\n",
        "parameters = {'C':[0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "lr = LogisticRegression(solver='liblinear', max_iter=1000)\n",
        "clf = GridSearchCV(lr, parameters, cv=5)\n",
        "clf.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Valutazione sul set di validazione\n",
        "best_model = clf.best_estimator_\n",
        "y_pred_val_prob = best_model.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_val = (y_pred_val_prob >= proba_threshold).astype(int)  # Applicazione della soglia\n",
        "val_report = classification_report(y_val, y_pred_val, target_names=classes_names)\n",
        "print(\"Classification Report on validation set:\")\n",
        "print(val_report)\n",
        "\n",
        "# Addestramento sul set di addestramento e di validazione combinato\n",
        "best_model.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Valutazione finale sul set di test\n",
        "y_pred_test_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_test = (y_pred_test_prob >= proba_threshold).astype(int)  # Applicazione della soglia\n",
        "test_report = classification_report(y_test, y_pred_test, target_names=classes_names)\n",
        "print(\"Classification Report on test set:\")\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRJ9AMLVjPRW"
      },
      "source": [
        "XGBoost SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpMQiJG39Pc4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# E' solo per ricordaci quali parametri possiamo settare in xgboost, non viene utilizzato\n",
        "possible_parameters = {\n",
        "    'objective': 'multi:softmax',  # Funzione obiettivo per problemi di classificazione multiclasse\n",
        "    'num_class': 2,  # Numero di classi nel tuo problema\n",
        "    'n_estimators': 200,  # Numero di alberi da addestrare\n",
        "    'learning_rate': 0.2,  # Tasso di apprendimento\n",
        "    'max_depth': 8,  # Massima profondità dell'albero\n",
        "    'min_child_weight': 1,  # Peso minimo richiesto per suddividere un nodo\n",
        "    'gamma': 0,  # Parametro di riduzione della perdita\n",
        "    'subsample': 0.8,  # Frazione di campioni da utilizzare per addestrare ciascun albero\n",
        "    'colsample_bytree': 0.8,  # Frazione di colonne da utilizzare per addestrare ciascun albero\n",
        "    'reg_alpha': 0,  # Parametro di regolarizzazione L1\n",
        "    'reg_lambda': 1,  # Parametro di regolarizzazione L2\n",
        "    'random_state': 42  # Seed per la riproducibilità dei risultati\n",
        "}\n",
        "\n",
        "# Definizione del modello XGBoost\n",
        "xgboost_model = xgb.XGBClassifier()\n",
        "\n",
        "# Definizione dei parametri da esplorare\n",
        "param_grid = {\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "}\n",
        "\n",
        "# Ricerca dei migliori iperparametri con GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgboost_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Visualizzazione dei migliori iperparametri trovati\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Valutazione sul set di validazione\n",
        "best_xgboost_model = grid_search.best_estimator_\n",
        "y_pred_val_prob = best_xgboost_model.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_val = (y_pred_val_prob >= xgboost_threshold).astype(int)  # Applicazione della soglia\n",
        "val_report = classification_report(y_val, y_pred_val, target_names=classes_names)\n",
        "print(\"Classification Report on validation set:\")\n",
        "print(val_report)\n",
        "\n",
        "# Valutazione sul set di test\n",
        "y_pred_test_prob = best_xgboost_model.predict_proba(X_test)[:, 1]  # Probabilità della classe positiva\n",
        "y_pred_test = (y_pred_test_prob >= xgboost_threshold).astype(int)  # Applicazione della soglia\n",
        "test_report = classification_report(y_test, y_pred_test, target_names=classes_names)\n",
        "print(\"Classification Report on test set:\")\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creazione della **curva ROC** per tutti i tre modelli che abbiamo visto:\n",
        "\n",
        "- La curva ROC è utile per valutare le prestazioni di un classificatore binario. Mostra il tasso di veri positivi (sensibilità) contro il tasso di falsi positivi (1-specificità). Un modello ideale avrà un'area sotto la curva (AUC) di ROC pari a 1, mentre un modello casuale avrà un'AUC di 0.5.\n",
        "\n",
        "- Non ha senso per il nostro scopo calcolarci la curva CMC"
      ],
      "metadata": {
        "id": "Gp_zXjc_sAoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_roc_curve(model, X_val, y_val):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
        "    else:\n",
        "        raise ValueError(\"Model doesn't have predict_proba method.\")\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curve for Logistic Regression\n",
        "plot_roc_curve(best_model_lr, X_val, y_val)\n",
        "\n",
        "# Plot ROC curve for SVM\n",
        "plot_roc_curve(best_model_svm, X_val, y_val)\n",
        "\n",
        "# Plot ROC curve for XGBoost\n",
        "plot_roc_curve(best_model_xgboost, X_val, y_val)"
      ],
      "metadata": {
        "id": "1WQ4hjAPuKsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}